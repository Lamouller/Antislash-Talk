#!/bin/bash
# Script de dÃ©ploiement VPS COMPLET avec toutes les corrections
set -e

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

# Configuration
PROJECT_DIR="/home/debian/antislash-talk"
REQUIRED_TOOLS=("docker" "git" "openssl" "curl" "jq" "envsubst")

print_header() {
    echo -e "\n${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${BLUE}    $1${NC}"
    echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

# Fonction pour gÃ©nÃ©rer des mots de passe sÃ©curisÃ©s
generate_password() {
    openssl rand -base64 32 | tr -d "=+/" | cut -c1-25
}

# VÃ©rifier qu'on est dans le bon rÃ©pertoire
if [ ! -f "$PROJECT_DIR/docker-compose.monorepo.yml" ]; then
    print_error "Ce script doit Ãªtre exÃ©cutÃ© depuis le rÃ©pertoire du projet"
    print_info "RÃ©pertoire attendu: $PROJECT_DIR"
    exit 1
fi

cd "$PROJECT_DIR"

# ASCII Art
echo -e "${CYAN}"
cat << "EOF"
    _          _   _     _           _       _____     _ _   
   / \   _ __ | |_(_)___| | __ _ ___| |__   |_   _|_ _| | | __
  / _ \ | '_ \| __| / __| |/ _` / __| '_ \    | |/ _` | | |/ /
 / ___ \| | | | |_| \__ \ | (_| \__ \ | | |   | | (_| | |   < 
/_/   \_\_| |_|\__|_|___/_|\__,_|___/_| |_|   |_|\__,_|_|_|\_\

EOF
echo -e "${NC}"
echo -e "${PURPLE}Script de DÃ©ploiement Complet Final v5.0 - HTTPS + Storage RLS${NC}"

# VÃ©rifier les permissions Docker
if ! docker ps >/dev/null 2>&1; then
    print_error "Permissions Docker manquantes"
    print_info "Ajout au groupe docker..."
    sudo usermod -aG docker $USER
    print_warning "Veuillez vous dÃ©connecter/reconnecter et relancer le script"
    exit 1
fi

print_header "Ã‰TAPE 1/11 : Configuration initiale"

# VÃ©rifier si un .env.monorepo existe dÃ©jÃ 
if [ -f ".env.monorepo" ]; then
    print_warning "Un fichier .env.monorepo existe dÃ©jÃ "
    echo ""
    read -p "Voulez-vous le conserver et sauter la configuration ? (oui/non) [non] : " KEEP_ENV
    if [ "${KEEP_ENV}" = "oui" ] || [ "${KEEP_ENV}" = "o" ] || [ "${KEEP_ENV}" = "yes" ] || [ "${KEEP_ENV}" = "y" ]; then
        print_success "Conservation de .env.monorepo existant"
        print_info "Passage directement au dÃ©ploiement..."
        
        # Charger les variables existantes
        source .env.monorepo
        
        # Passer Ã  l'Ã©tape de dÃ©ploiement (ligne aprÃ¨s gÃ©nÃ©ration des clÃ©s)
        SKIP_CONFIG=true
    else
        print_info "Suppression de l'ancien .env.monorepo"
        rm -f .env.monorepo
        SKIP_CONFIG=false
    fi
else
    SKIP_CONFIG=false
fi

# Si on garde la config existante, sauter la configuration
if [ "$SKIP_CONFIG" = "true" ]; then
    print_info "Saut de la configuration interactive"
    # S'assurer que VPS_HOST est dÃ©fini
    if [ -z "$VPS_HOST" ]; then
        VPS_HOST=$(grep "^VPS_HOST=" .env.monorepo | cut -d= -f2 || echo "")
    fi
    # Extraire les variables nÃ©cessaires
    STUDIO_PASSWORD=$(grep "^STUDIO_PASSWORD=" .env.monorepo | cut -d= -f2 || generate_password)
    APP_USER_EMAIL=$(grep "^APP_USER_EMAIL=" .env.monorepo | cut -d= -f2 || echo "admin@antislash-talk.fr")
    APP_USER_PASSWORD=$(grep "^APP_USER_PASSWORD=" .env.monorepo | cut -d= -f2 || generate_password)
    
    print_success "Configuration chargÃ©e depuis .env.monorepo"
    
    # Passer directement Ã  l'Ã©tape 4 (build)
    GOTO_BUILD=true
else
    GOTO_BUILD=false
fi

# DÃ©tecter l'IP automatiquement
if [ "$GOTO_BUILD" != "true" ]; then
    print_info "DÃ©tection de l'IP du VPS..."
    DETECTED_IP=""

    # MÃ©thode 1: curl ifconfig.me (forcer IPv4)
    if [ -z "$DETECTED_IP" ]; then
        DETECTED_IP=$(curl -4 -s --max-time 5 ifconfig.me 2>/dev/null | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$' || true)
    fi

# MÃ©thode 2: curl ipinfo.io (forcer IPv4)
if [ -z "$DETECTED_IP" ]; then
    DETECTED_IP=$(curl -4 -s --max-time 5 ipinfo.io/ip 2>/dev/null | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$' || true)
fi

# MÃ©thode 3: curl checkip.amazonaws.com (forcer IPv4)
if [ -z "$DETECTED_IP" ]; then
    DETECTED_IP=$(curl -4 -s --max-time 5 checkip.amazonaws.com 2>/dev/null | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$' || true)
fi

# MÃ©thode 3bis: curl spÃ©cifique IPv4
if [ -z "$DETECTED_IP" ]; then
    DETECTED_IP=$(curl -4 -s --max-time 5 ipv4.icanhazip.com 2>/dev/null | tr -d '\n' || true)
fi

# MÃ©thode 4: ip addr show (IPv4 seulement)
if [ -z "$DETECTED_IP" ]; then
    DETECTED_IP=$(ip addr show | grep -oP '(?<=inet\s)\d+(\.\d+){3}' | grep -v '127.0.0.1' | grep -v '^172\.' | grep -v '^10\.' | grep -v '^192\.168\.' | head -1 || true)
fi

# MÃ©thode 5: hostname -I (IPv4 seulement)
if [ -z "$DETECTED_IP" ]; then
    DETECTED_IP=$(hostname -I | tr ' ' '\n' | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$' | grep -v '^127\.' | grep -v '^172\.' | grep -v '^10\.' | grep -v '^192\.168\.' | head -1 || true)
fi

# MÃ©thode 6: wget -qO- (en cas d'Ã©chec des autres)
if [ -z "$DETECTED_IP" ]; then
    DETECTED_IP=$(wget -qO- --timeout=5 http://ipv4.icanhazip.com 2>/dev/null | tr -d '\n' || true)
fi

# Valider que c'est bien une IPv4
if [ -n "$DETECTED_IP" ]; then
    # VÃ©rifier le format IPv4
    if echo "$DETECTED_IP" | grep -qE '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$'; then
        print_success "IPv4 dÃ©tectÃ©e : $DETECTED_IP"
        read -p "IP ou domaine du VPS [$DETECTED_IP] : " VPS_HOST
        VPS_HOST=${VPS_HOST:-$DETECTED_IP}
    else
        print_warning "IP dÃ©tectÃ©e non valide (IPv6?) : $DETECTED_IP"
        print_info "Tentative de dÃ©tection IPv4 forcÃ©e..."
        
        # DerniÃ¨re tentative avec curl spÃ©cifique IPv4
        DETECTED_IP=$(curl -4 -s --max-time 5 api.ipify.org 2>/dev/null || true)
        
        if [ -n "$DETECTED_IP" ] && echo "$DETECTED_IP" | grep -qE '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$'; then
            print_success "IPv4 trouvÃ©e : $DETECTED_IP"
            read -p "IP ou domaine du VPS [$DETECTED_IP] : " VPS_HOST
            VPS_HOST=${VPS_HOST:-$DETECTED_IP}
        else
            VPS_HOST=""
            while [ -z "$VPS_HOST" ]; do
                read -p "IP ou domaine du VPS (entrez votre IPv4) : " VPS_HOST
            done
        fi
    fi
else
    print_warning "Impossible de dÃ©tecter l'IP automatiquement"
    VPS_HOST=""
    while [ -z "$VPS_HOST" ]; do
        read -p "IP ou domaine du VPS : " VPS_HOST
    done
fi

# Studio password
print_info "GÃ©nÃ©ration d'un mot de passe sÃ©curisÃ© pour Studio..."
GENERATED_STUDIO_PASSWORD=$(generate_password)
print_success "Mot de passe gÃ©nÃ©rÃ© : $GENERATED_STUDIO_PASSWORD"

read -sp "Mot de passe pour Supabase Studio [$GENERATED_STUDIO_PASSWORD] : " STUDIO_PASSWORD
echo
STUDIO_PASSWORD=${STUDIO_PASSWORD:-$GENERATED_STUDIO_PASSWORD}

# Email admin
read -p "Email de l'utilisateur admin [admin@antislash-talk.fr] : " APP_USER_EMAIL
APP_USER_EMAIL=${APP_USER_EMAIL:-admin@antislash-talk.fr}

# Mot de passe admin
print_info "GÃ©nÃ©ration d'un mot de passe sÃ©curisÃ© pour l'utilisateur admin..."
GENERATED_ADMIN_PASSWORD=$(generate_password)
print_success "Mot de passe gÃ©nÃ©rÃ© : $GENERATED_ADMIN_PASSWORD"

read -sp "Mot de passe pour l'utilisateur admin [$GENERATED_ADMIN_PASSWORD] : " APP_USER_PASSWORD
echo
APP_USER_PASSWORD=${APP_USER_PASSWORD:-$GENERATED_ADMIN_PASSWORD}

# Hide marketing pages
read -p "Masquer les pages marketing ? (oui/non) [oui] : " HIDE_MARKETING
VITE_HIDE_MARKETING_PAGES=$([ "${HIDE_MARKETING:-oui}" = "oui" ] && echo "true" || echo "false")

# HuggingFace token
read -p "Token HuggingFace (optionnel, EntrÃ©e pour ignorer) : " HUGGINGFACE_TOKEN

fi # Fin du if [ "$GOTO_BUILD" != "true" ]

print_header "Ã‰TAPE 2/11 : GÃ©nÃ©ration des clÃ©s et mots de passe"

# Si on saute la config, charger les clÃ©s depuis .env.monorepo
if [ "$GOTO_BUILD" = "true" ]; then
    print_info "Chargement des clÃ©s depuis .env.monorepo..."
    POSTGRES_PASSWORD=$(grep "^POSTGRES_PASSWORD=" .env.monorepo | cut -d= -f2)
    JWT_SECRET=$(grep "^JWT_SECRET=" .env.monorepo | cut -d= -f2)
    ANON_KEY=$(grep "^ANON_KEY=" .env.monorepo | cut -d= -f2)
    SERVICE_ROLE_KEY=$(grep "^SERVICE_ROLE_KEY=" .env.monorepo | cut -d= -f2)
    print_success "ClÃ©s chargÃ©es"
else

# GÃ©nÃ©rer toutes les clÃ©s nÃ©cessaires
POSTGRES_PASSWORD=$(generate_password)
JWT_SECRET=$(openssl rand -base64 64 | tr -d '\n')
ANON_KEY=""
SERVICE_ROLE_KEY=""

# GÃ©nÃ©rer les clÃ©s JWT avec le script Node.js
print_info "GÃ©nÃ©ration des clÃ©s JWT..."
cat > generate-jwt-keys.mjs << 'EOF'
import jwt from 'jsonwebtoken';

const jwtSecret = process.env.JWT_SECRET;
const expiresIn = '10y';

const anonPayload = {
  role: 'anon',
  iss: 'supabase',
  iat: Math.floor(Date.now() / 1000),
  exp: Math.floor(Date.now() / 1000) + (10 * 365 * 24 * 60 * 60)
};

const serviceRolePayload = {
  role: 'service_role',
  iss: 'supabase',
  iat: Math.floor(Date.now() / 1000),
  exp: Math.floor(Date.now() / 1000) + (10 * 365 * 24 * 60 * 60)
};

const anonKey = jwt.sign(anonPayload, jwtSecret);
const serviceRoleKey = jwt.sign(serviceRolePayload, jwtSecret);

console.log(`ANON_KEY=${anonKey}`);
console.log(`SERVICE_ROLE_KEY=${serviceRoleKey}`);
EOF

# Installer jsonwebtoken temporairement et gÃ©nÃ©rer les clÃ©s
print_info "Installation temporaire de jsonwebtoken..."
# Sauvegarder l'ancien package.json s'il existe
if [ -f "package.json" ]; then
    cp package.json package.json.backup
fi

# CrÃ©er un package.json propre pour l'installation
cat > package.json << 'PKGJSON'
{
  "type": "module",
  "dependencies": {
    "jsonwebtoken": "^9.0.0"
  }
}
PKGJSON

# Installer pour de vrai
npm install >/dev/null 2>&1

# ExÃ©cuter le script pour obtenir les clÃ©s
JWT_OUTPUT=$(JWT_SECRET="$JWT_SECRET" node generate-jwt-keys.mjs 2>&1)
JWT_EXIT_CODE=$?

# VÃ©rifier que Ã§a a fonctionnÃ©
if [ $JWT_EXIT_CODE -ne 0 ] || [ -z "$JWT_OUTPUT" ] || echo "$JWT_OUTPUT" | grep -q "Error"; then
    print_error "Erreur lors de la gÃ©nÃ©ration des clÃ©s JWT"
    echo "$JWT_OUTPUT"
    exit 1
fi

ANON_KEY=$(echo "$JWT_OUTPUT" | grep "ANON_KEY=" | cut -d'=' -f2-)
SERVICE_ROLE_KEY=$(echo "$JWT_OUTPUT" | grep "SERVICE_ROLE_KEY=" | cut -d'=' -f2-)

# VÃ©rifier que les clÃ©s ont bien Ã©tÃ© gÃ©nÃ©rÃ©es
if [ -z "$ANON_KEY" ] || [ -z "$SERVICE_ROLE_KEY" ]; then
    print_error "Les clÃ©s JWT n'ont pas pu Ãªtre extraites"
    echo "JWT_OUTPUT:"
    echo "$JWT_OUTPUT"
    exit 1
fi

# Nettoyer
rm -f generate-jwt-keys.mjs
rm -rf node_modules package-lock.json

# Restaurer l'ancien package.json s'il existait
if [ -f "package.json.backup" ]; then
    mv package.json.backup package.json
else
    rm -f package.json
fi

print_success "ClÃ©s gÃ©nÃ©rÃ©es avec succÃ¨s"

fi # Fin du else (gÃ©nÃ©ration des clÃ©s)

print_header "Ã‰TAPE 3/11 : CrÃ©ation du fichier .env.monorepo"

# Si on a gardÃ© la config, skip cette Ã©tape aussi
if [ "$GOTO_BUILD" = "true" ]; then
    print_info "Fichier .env.monorepo dÃ©jÃ  existant, Ã©tape ignorÃ©e"
else

# CrÃ©er le fichier .env.monorepo avec toutes les variables
cat > .env.monorepo << EOF
# Configuration de base
NODE_ENV=production
VPS_HOST=${VPS_HOST}
API_EXTERNAL_URL=https://${VPS_HOST}:8443
VITE_SUPABASE_URL=https://${VPS_HOST}:8443
VITE_SUPABASE_ANON_KEY=${ANON_KEY}
VITE_HIDE_MARKETING_PAGES=${VITE_HIDE_MARKETING_PAGES}
VITE_OLLAMA_URL=https://${VPS_HOST}:8445

# PostgreSQL
POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
POSTGRES_HOST=db
POSTGRES_PORT=5432
POSTGRES_DB=postgres

# JWT
JWT_SECRET=${JWT_SECRET}
ANON_KEY=${ANON_KEY}
SERVICE_ROLE_KEY=${SERVICE_ROLE_KEY}

# URLs des services
SUPABASE_URL=http://${VPS_HOST}:54321
PUBLIC_REST_URL=http://${VPS_HOST}:54321/rest/v1/
PGRST_JWT_SECRET=${JWT_SECRET}

# Auth Service (GoTrue)
GOTRUE_SITE_URL=http://${VPS_HOST}
GOTRUE_URI_ALLOW_LIST=http://${VPS_HOST}:*,http://${VPS_HOST}
GOTRUE_JWT_SECRET=${JWT_SECRET}
GOTRUE_JWT_EXP=3600
GOTRUE_JWT_DEFAULT_GROUP_NAME=authenticated
GOTRUE_DB_DRIVER=postgres
DATABASE_URL=postgres://supabase_auth_admin:${POSTGRES_PASSWORD}@db:5432/postgres

# Studio
STUDIO_DEFAULT_ORGANIZATION="Antislash Talk"
STUDIO_DEFAULT_PROJECT=Production
STUDIO_PORT=3000
STUDIO_PROXY_PORT=54327
SUPABASE_PUBLIC_URL=http://${VPS_HOST}:54321
NEXT_PUBLIC_ENABLE_LOGS=true
NEXT_ANALYTICS_BACKEND_PROVIDER=postgres

# Storage
STORAGE_BACKEND=file
STORAGE_FILE_BACKEND_PATH=/var/lib/storage
STORAGE_S3_BUCKET=
STORAGE_S3_ENDPOINT=
STORAGE_S3_FORCE_PATH_STYLE=
STORAGE_S3_PROTOCOL=
STORAGE_S3_REGION=

# Email (Local)
GOTRUE_SMTP_HOST=inbucket
GOTRUE_SMTP_PORT=2500
GOTRUE_SMTP_USER=
GOTRUE_SMTP_PASS=
GOTRUE_SMTP_SENDER_NAME="Antislash Talk"

# Configuration utilisateur
APP_USER_EMAIL=${APP_USER_EMAIL}
APP_USER_PASSWORD=${APP_USER_PASSWORD}

# Kong
KONG_HTTP2_MAX_FIELD_SIZE=16384
KONG_HTTP2_MAX_HEADER_SIZE=16384

# Dashboard
DASHBOARD_USERNAME=antislash
DASHBOARD_PASSWORD=${STUDIO_PASSWORD}

# Studio Auth
STUDIO_USER=antislash
STUDIO_PASSWORD=${STUDIO_PASSWORD}

# AI Services
HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
OLLAMA_BASE_URL=http://ollama:11434
VITE_OLLAMA_BASE_URL=http://${VPS_HOST}:11434

# Imgproxy
IMGPROXY_ENABLE_WEBP_DETECTION=true
EOF

print_success "Configuration crÃ©Ã©e"

fi # Fin du else (crÃ©ation de .env.monorepo)

print_header "Ã‰TAPE 4/11 : ArrÃªt des services existants"

# ArrÃªter et nettoyer les services existants
docker compose -f docker-compose.monorepo.yml down -v --remove-orphans || true
docker system prune -f

print_header "Ã‰TAPE 5/11 : Construction de l'image web"

print_info "CrÃ©ation du fichier apps/web/.env pour le build..."
cat > apps/web/.env << EOF
VITE_SUPABASE_URL=https://${VPS_HOST}:8443
VITE_SUPABASE_ANON_KEY=${ANON_KEY}
VITE_HIDE_MARKETING_PAGES=${VITE_HIDE_MARKETING_PAGES}
VITE_OLLAMA_URL=https://${VPS_HOST}:8445
EOF

print_info "Export des variables pour le build..."
export API_EXTERNAL_URL="https://${VPS_HOST}:8443"
export VITE_SUPABASE_URL="https://${VPS_HOST}:8443"
export VITE_SUPABASE_ANON_KEY="$ANON_KEY"
export VITE_HIDE_MARKETING_PAGES="$VITE_HIDE_MARKETING_PAGES"
export VITE_OLLAMA_URL="https://${VPS_HOST}:8445"

print_info "Construction de l'image web..."
docker compose -f docker-compose.monorepo.yml --env-file .env.monorepo build \
  --build-arg VITE_SUPABASE_URL="https://${VPS_HOST}:8443" \
  --build-arg VITE_SUPABASE_ANON_KEY="${ANON_KEY}" \
  --build-arg VITE_HIDE_MARKETING_PAGES="${VITE_HIDE_MARKETING_PAGES}" \
  --build-arg VITE_OLLAMA_URL="https://${VPS_HOST}:8445" \
  web

print_header "Ã‰TAPE 6/11 : DÃ©marrage de PostgreSQL"

print_info "DÃ©marrage de PostgreSQL seul..."
docker compose -f docker-compose.monorepo.yml --env-file .env.monorepo up -d db

# Attendre que PostgreSQL soit prÃªt
print_info "Attente de PostgreSQL..."
POSTGRES_READY=false
for i in {1..30}; do
    if docker exec antislash-talk-db pg_isready -U postgres > /dev/null 2>&1; then
        POSTGRES_READY=true
        break
    fi
    sleep 2
done

if [ "$POSTGRES_READY" = false ]; then
    print_error "PostgreSQL n'a pas dÃ©marrÃ©"
    exit 1
fi

print_success "PostgreSQL prÃªt"
sleep 5

print_header "Ã‰TAPE 7/11 : Configuration de PostgreSQL"

print_info "Configuration complÃ¨te de PostgreSQL..."
docker exec -i antislash-talk-db psql -U postgres << EOF
-- Configuration
ALTER SYSTEM SET password_encryption = 'scram-sha-256';
SELECT pg_reload_conf();

-- CrÃ©ation des schÃ©mas
CREATE SCHEMA IF NOT EXISTS auth;
CREATE SCHEMA IF NOT EXISTS storage;
CREATE SCHEMA IF NOT EXISTS extensions;
CREATE SCHEMA IF NOT EXISTS public;

-- Extensions (avec gestion d'erreur)
DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS "uuid-ossp" WITH SCHEMA extensions CASCADE;
EXCEPTION
    WHEN others THEN
        RAISE NOTICE 'Extension uuid-ossp: %', SQLERRM;
END \$\$;

DO \$\$
BEGIN
    CREATE EXTENSION IF NOT EXISTS pgcrypto WITH SCHEMA extensions CASCADE;
EXCEPTION
    WHEN others THEN
        RAISE NOTICE 'Extension pgcrypto: %', SQLERRM;
END \$\$;

-- Types ENUM pour GoTrue MFA (DOIVENT Ãªtre dans public, pas auth !)
DO \$\$ 
BEGIN
    -- factor_type dans public
    IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'factor_type' AND typnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'public')) THEN
        CREATE TYPE factor_type AS ENUM ('totp', 'webauthn', 'phone');
    END IF;
    
    -- factor_status dans public
    IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'factor_status' AND typnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'public')) THEN
        CREATE TYPE factor_status AS ENUM ('unverified', 'verified');
    END IF;
    
    -- aal_level dans public
    IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'aal_level' AND typnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'public')) THEN
        CREATE TYPE aal_level AS ENUM ('aal1', 'aal2', 'aal3');
    END IF;
END \$\$;

-- Table auth.schema_migrations
CREATE TABLE IF NOT EXISTS auth.schema_migrations (
    version text NOT NULL PRIMARY KEY
);

-- CrÃ©ation et configuration des rÃ´les avec EXECUTE format pour Ã©viter les erreurs de syntaxe
DO \$\$
DECLARE
    db_password text := '${POSTGRES_PASSWORD}';
BEGIN
    -- postgres
    EXECUTE format('ALTER ROLE postgres PASSWORD %L', db_password);
    
    -- supabase_auth_admin (SUPERUSER pour Ã©viter les problÃ¨mes de permissions)
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'supabase_auth_admin') THEN
        CREATE ROLE supabase_auth_admin;
    END IF;
    EXECUTE format('ALTER ROLE supabase_auth_admin WITH SUPERUSER CREATEDB CREATEROLE LOGIN PASSWORD %L', db_password);
    
    -- supabase_storage_admin (SUPERUSER aussi)
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'supabase_storage_admin') THEN
        CREATE ROLE supabase_storage_admin;
    END IF;
    EXECUTE format('ALTER ROLE supabase_storage_admin WITH SUPERUSER CREATEDB CREATEROLE LOGIN PASSWORD %L', db_password);
    
    -- authenticator
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'authenticator') THEN
        CREATE ROLE authenticator;
    END IF;
    EXECUTE format('ALTER ROLE authenticator WITH LOGIN PASSWORD %L', db_password);
    
    -- service_role
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'service_role') THEN
        CREATE ROLE service_role;
    END IF;
    ALTER ROLE service_role WITH NOLOGIN;
    
    -- anon
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'anon') THEN
        CREATE ROLE anon;
    END IF;
    ALTER ROLE anon WITH NOLOGIN;
    
    -- supabase_admin
    IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'supabase_admin') THEN
        CREATE ROLE supabase_admin;
    END IF;
    EXECUTE format('ALTER ROLE supabase_admin WITH SUPERUSER CREATEDB CREATEROLE LOGIN PASSWORD %L', db_password);
END \$\$;

-- Permissions et ownership
ALTER SCHEMA auth OWNER TO supabase_auth_admin;
ALTER SCHEMA storage OWNER TO supabase_storage_admin;
ALTER TABLE auth.schema_migrations OWNER TO supabase_auth_admin;

-- CRITIQUE: Configurer le search_path pour supabase_auth_admin
ALTER ROLE supabase_auth_admin SET search_path TO auth, public, extensions;

GRANT ALL PRIVILEGES ON SCHEMA auth TO supabase_auth_admin;
GRANT ALL PRIVILEGES ON SCHEMA storage TO supabase_storage_admin;
GRANT ALL PRIVILEGES ON SCHEMA public TO postgres, supabase_admin;
GRANT ALL PRIVILEGES ON DATABASE postgres TO supabase_admin, supabase_auth_admin, supabase_storage_admin;
GRANT USAGE ON SCHEMA auth TO authenticator;
GRANT USAGE ON SCHEMA storage TO authenticator;
GRANT USAGE ON SCHEMA public TO authenticator, anon, service_role;
GRANT USAGE ON SCHEMA extensions TO postgres, supabase_admin, supabase_auth_admin, supabase_storage_admin;
GRANT anon TO authenticator;
GRANT service_role TO authenticator;

SELECT 'PostgreSQL configurÃ©' as status;
EOF

# Configurer pg_hba.conf
print_info "Configuration de pg_hba.conf..."
docker exec antislash-talk-db bash -c "cat > /var/lib/postgresql/data/pg_hba.conf << 'EOF'
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             all                                     trust
host    all             all             127.0.0.1/32            scram-sha-256
host    all             all             ::1/128                 scram-sha-256
host    all             all             0.0.0.0/0               scram-sha-256
host    all             all             ::/0                    scram-sha-256
EOF"

docker exec antislash-talk-db psql -U postgres -c "SELECT pg_reload_conf();"
print_success "PostgreSQL configurÃ©"

print_header "Ã‰TAPE 8/11 : Application des migrations et dÃ©marrage des services"

# Appliquer les migrations
print_info "Application des migrations..."
for migration in packages/supabase/migrations/*.sql; do
    if [ -f "$migration" ]; then
        filename=$(basename "$migration")
        if docker exec -i antislash-talk-db psql -U postgres -d postgres < "$migration" 2>/dev/null; then
            print_success "Migration appliquÃ©e : $filename"
        else
            print_info "Migration dÃ©jÃ  appliquÃ©e ou non nÃ©cessaire : $filename"
        fi
    fi
done

# DÃ©marrer tous les services
print_info "DÃ©marrage de tous les services..."
docker compose -f docker-compose.monorepo.yml --env-file .env.monorepo up -d

# Attendre que les services critiques soient prÃªts
print_info "Attente du dÃ©marrage des services..."

# VÃ©rifier que Auth est prÃªt
print_info "VÃ©rification du service Auth..."
AUTH_READY=false
for i in {1..30}; do
    if docker exec antislash-talk-auth curl -f http://localhost:9999/health 2>/dev/null | grep -q "ok"; then
        AUTH_READY=true
        print_success "Service Auth prÃªt"
        break
    fi
    sleep 2
done

if [ "$AUTH_READY" = false ]; then
    print_warning "Service Auth pas encore prÃªt aprÃ¨s 60s, on continue quand mÃªme..."
fi

# VÃ©rifier que Storage est prÃªt
print_info "VÃ©rification du service Storage..."
STORAGE_READY=false
for i in {1..30}; do
    if docker exec antislash-talk-storage curl -f http://localhost:5000/status 2>/dev/null | grep -q "ok"; then
        STORAGE_READY=true
        print_success "Service Storage prÃªt"
        break
    fi
    sleep 2
done

if [ "$STORAGE_READY" = false ]; then
    print_warning "Service Storage pas encore prÃªt aprÃ¨s 60s, on continue quand mÃªme..."
fi

# Attendre encore un peu pour la stabilisation
print_info "Attente de stabilisation (15s)..."
sleep 15

# CRITIQUE: Mettre Ã  jour Kong avec les bonnes clÃ©s
print_info "Mise Ã  jour de Kong avec les clÃ©s JWT..."

# ArrÃªter Kong temporairement
docker stop antislash-talk-kong

# CrÃ©er le nouveau fichier kong.yml avec les bonnes clÃ©s
cp packages/supabase/kong.yml packages/supabase/kong.yml.backup
sed -i "s/ANON_KEY_PLACEHOLDER/${ANON_KEY}/g" packages/supabase/kong.yml
sed -i "s/SERVICE_ROLE_KEY_PLACEHOLDER/${SERVICE_ROLE_KEY}/g" packages/supabase/kong.yml

# RedÃ©marrer Kong qui va charger le nouveau fichier
docker start antislash-talk-kong

# Attendre que Kong soit prÃªt
sleep 5
print_success "Kong mis Ã  jour avec les nouvelles clÃ©s"

# Restaurer le template pour les prochains dÃ©ploiements
mv packages/supabase/kong.yml.backup packages/supabase/kong.yml 2>/dev/null || true

print_header "Ã‰TAPE 9/10 : Configuration Nginx HTTPS"

print_info "Installation de Nginx si nÃ©cessaire..."
if ! command -v nginx &> /dev/null; then
    sudo apt-get update
    sudo apt-get install -y nginx
fi

print_info "GÃ©nÃ©ration des certificats SSL auto-signÃ©s..."
sudo mkdir -p /etc/nginx/ssl
if [ ! -f /etc/nginx/ssl/selfsigned.crt ]; then
    sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
        -keyout /etc/nginx/ssl/selfsigned.key \
        -out /etc/nginx/ssl/selfsigned.crt \
        -subj "/C=FR/ST=France/L=Paris/O=Antislash/CN=${VPS_HOST}"
    print_success "Certificats SSL gÃ©nÃ©rÃ©s"
else
    print_info "Certificats SSL dÃ©jÃ  existants"
fi

print_info "Configuration de Nginx pour HTTPS..."
sudo tee /etc/nginx/sites-available/antislash-talk-ssl > /dev/null << 'NGINXCONF'
# Redirection HTTP vers HTTPS
server {
    listen 80;
    server_name _;
    return 301 https://$host$request_uri;
}

# Application Web (HTTPS sur 443)
server {
    listen 443 ssl http2;
    server_name _;

    ssl_certificate /etc/nginx/ssl/selfsigned.crt;
    ssl_certificate_key /etc/nginx/ssl/selfsigned.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }
}

# API Supabase (HTTPS sur 8443)
server {
    listen 8443 ssl http2;
    server_name _;

    ssl_certificate /etc/nginx/ssl/selfsigned.crt;
    ssl_certificate_key /etc/nginx/ssl/selfsigned.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;

    client_max_body_size 100M;

    location / {
        proxy_pass http://localhost:54321;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
        proxy_read_timeout 300s;
        proxy_connect_timeout 75s;
    }
}

# Studio Supabase (HTTPS sur 8444)
server {
    listen 8444 ssl http2;
    server_name _;

    ssl_certificate /etc/nginx/ssl/selfsigned.crt;
    ssl_certificate_key /etc/nginx/ssl/selfsigned.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;

    location / {
        proxy_pass http://localhost:54327;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }
}

# Ollama API (HTTPS sur 8445)
server {
    listen 8445 ssl http2;
    server_name _;

    ssl_certificate /etc/nginx/ssl/selfsigned.crt;
    ssl_certificate_key /etc/nginx/ssl/selfsigned.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;

    client_max_body_size 500M;
    proxy_read_timeout 600s;
    proxy_connect_timeout 600s;

    location / {
        proxy_pass http://localhost:11434;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }
}
NGINXCONF

# Activer le site
sudo ln -sf /etc/nginx/sites-available/antislash-talk-ssl /etc/nginx/sites-enabled/
sudo rm -f /etc/nginx/sites-enabled/default

# Tester et recharger Nginx
print_info "Test de la configuration Nginx..."
if sudo nginx -t; then
    print_success "Configuration Nginx valide"
    sudo systemctl reload nginx
    print_success "Nginx rechargÃ© avec HTTPS"
else
    print_error "Erreur dans la configuration Nginx"
    exit 1
fi

print_header "Ã‰TAPE 10/11 : CrÃ©ation des donnÃ©es initiales"

# Attendre que les tables existent (crÃ©Ã©es par les migrations)
print_info "Attente de la crÃ©ation des tables par les migrations..."
TABLES_READY=false
for i in {1..30}; do
    AUTH_EXISTS=$(docker exec antislash-talk-db psql -U postgres -t -c "SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'auth' AND table_name = 'users');" | tr -d ' ')
    STORAGE_EXISTS=$(docker exec antislash-talk-db psql -U postgres -t -c "SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'storage' AND table_name = 'buckets');" | tr -d ' ')
    PROFILES_EXISTS=$(docker exec antislash-talk-db psql -U postgres -t -c "SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'profiles');" | tr -d ' ')
    
    if [ "$AUTH_EXISTS" = "t" ] && [ "$STORAGE_EXISTS" = "t" ] && [ "$PROFILES_EXISTS" = "t" ]; then
        TABLES_READY=true
        print_success "Toutes les tables nÃ©cessaires sont crÃ©Ã©es (auth.users, storage.buckets, public.profiles)"
        break
    fi
    sleep 2
done

if [ "$TABLES_READY" = true ]; then
    print_success "Tables crÃ©Ã©es, ajout des donnÃ©es initiales..."
    
    docker exec -i antislash-talk-db psql -U postgres << EOF
-- DÃ©sactiver temporairement RLS
ALTER TABLE auth.users DISABLE ROW LEVEL SECURITY;
ALTER TABLE auth.identities DISABLE ROW LEVEL SECURITY;
ALTER TABLE storage.buckets DISABLE ROW LEVEL SECURITY;
ALTER TABLE public.profiles DISABLE ROW LEVEL SECURITY;

-- CrÃ©er l'utilisateur admin (seulement s'il n'existe pas dÃ©jÃ )
DO \$\$
DECLARE
    v_user_id uuid;
    v_user_email text := '${APP_USER_EMAIL}';
BEGIN
    -- VÃ©rifier si l'utilisateur existe dÃ©jÃ 
    SELECT id INTO v_user_id FROM auth.users WHERE email = v_user_email;
    
    IF v_user_id IS NULL THEN
        -- CrÃ©er l'utilisateur
        INSERT INTO auth.users (
            id,
            email,
            encrypted_password,
            email_confirmed_at,
            created_at,
            updated_at,
            instance_id,
            aud,
            role,
            raw_app_meta_data,
            raw_user_meta_data
        ) VALUES (
            extensions.gen_random_uuid(),
            v_user_email,
            extensions.crypt('${APP_USER_PASSWORD}', extensions.gen_salt('bf', 6)),
            now(),
            now(),
            now(),
            '00000000-0000-0000-0000-000000000000',
            'authenticated',
            'authenticated',
            '{"provider": "email", "providers": ["email"]}',
            '{}'
        ) RETURNING id INTO v_user_id;
        
        -- CrÃ©er l'identitÃ©
        INSERT INTO auth.identities (
            id,
            user_id,
            provider_id,
            provider,
            identity_data,
            last_sign_in_at,
            created_at,
            updated_at
        ) VALUES (
            extensions.gen_random_uuid(),
            v_user_id,
            v_user_id::text,
            'email',
            json_build_object('sub', v_user_id::text, 'email', v_user_email)::jsonb,
            now(),
            now(),
            now()
        );
        
        RAISE NOTICE 'Utilisateur crÃ©Ã©: %', v_user_email;
    ELSE
        RAISE NOTICE 'Utilisateur existe dÃ©jÃ : %', v_user_email;
    END IF;
END \$\$;

-- CrÃ©er le profil pour l'utilisateur admin
INSERT INTO public.profiles (id, email, full_name, role)
SELECT id, email, email, 'admin'
FROM auth.users
WHERE email = '${APP_USER_EMAIL}'
ON CONFLICT (id) DO UPDATE SET 
    email = EXCLUDED.email,
    role = 'admin',
    updated_at = now();

-- CRITIQUE: Corriger les colonnes NULL en auth.users (GoTrue attend des strings vides, pas NULL)
UPDATE auth.users SET 
    email_change = COALESCE(email_change, ''),
    email_change_token_new = COALESCE(email_change_token_new, ''),
    email_change_token_current = COALESCE(email_change_token_current, ''),
    confirmation_token = COALESCE(confirmation_token, ''),
    recovery_token = COALESCE(recovery_token, ''),
    phone_change = COALESCE(phone_change, ''),
    phone_change_token = COALESCE(phone_change_token, '')
WHERE email_change IS NULL 
   OR email_change_token_new IS NULL 
   OR email_change_token_current IS NULL
   OR confirmation_token IS NULL
   OR recovery_token IS NULL
   OR phone_change IS NULL
   OR phone_change_token IS NULL;

-- DÃ©finir DEFAULT '' pour Ã©viter les NULL futurs
ALTER TABLE auth.users ALTER COLUMN email_change SET DEFAULT '';
ALTER TABLE auth.users ALTER COLUMN email_change_token_new SET DEFAULT '';
ALTER TABLE auth.users ALTER COLUMN email_change_token_current SET DEFAULT '';
ALTER TABLE auth.users ALTER COLUMN confirmation_token SET DEFAULT '';
ALTER TABLE auth.users ALTER COLUMN recovery_token SET DEFAULT '';
ALTER TABLE auth.users ALTER COLUMN phone_change SET DEFAULT '';
ALTER TABLE auth.users ALTER COLUMN phone_change_token SET DEFAULT '';

-- Note: Les tables (profiles, meetings, api_keys, user_api_keys, etc.) sont crÃ©Ã©es par les migrations
-- Ne pas les crÃ©er manuellement ici pour Ã©viter les conflits

-- CrÃ©er tous les buckets nÃ©cessaires (selon base locale rÃ©elle)
INSERT INTO storage.buckets (id, name, public, file_size_limit, allowed_mime_types, created_at, updated_at)
VALUES 
  ('avatars', 'avatars', true, NULL, NULL, now(), now()),
  ('meeting-audio', 'meeting-audio', true, NULL, NULL, now(), now()),
  ('meetingrecordings', 'meetingrecordings', true, NULL, NULL, now(), now()),
  ('reports', 'reports', false, NULL, NULL, now(), now()),
  ('transcriptions', 'transcriptions', false, NULL, NULL, now(), now())
ON CONFLICT (id) DO NOTHING;

-- Configurer les permissions Storage pour que tous les services puissent accÃ©der
GRANT USAGE ON SCHEMA storage TO postgres, anon, authenticated, service_role, supabase_storage_admin;
GRANT ALL ON ALL TABLES IN SCHEMA storage TO postgres, service_role, supabase_storage_admin;
GRANT ALL ON ALL SEQUENCES IN SCHEMA storage TO postgres, service_role, supabase_storage_admin;
GRANT ALL ON ALL TABLES IN SCHEMA storage TO authenticated;
GRANT ALL ON ALL SEQUENCES IN SCHEMA storage TO authenticated;
ALTER DEFAULT PRIVILEGES FOR ROLE postgres IN SCHEMA storage GRANT ALL ON TABLES TO authenticated, supabase_storage_admin;
ALTER DEFAULT PRIVILEGES FOR ROLE postgres IN SCHEMA storage GRANT ALL ON SEQUENCES TO authenticated, supabase_storage_admin;

-- RÃ©activer RLS avec FORCE pour s'assurer que les policies s'appliquent
ALTER TABLE auth.users ENABLE ROW LEVEL SECURITY;
ALTER TABLE auth.identities ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.profiles ENABLE ROW LEVEL SECURITY;
ALTER TABLE storage.buckets FORCE ROW LEVEL SECURITY;
ALTER TABLE storage.objects FORCE ROW LEVEL SECURITY;

-- CrÃ©er les policies RLS
DO \$\$
BEGIN
    -- Policies pour auth.users
    DROP POLICY IF EXISTS "Users can view own profile" ON auth.users;
    CREATE POLICY "Users can view own profile" 
    ON auth.users FOR SELECT 
    USING (auth.uid() = id);
    
    -- Policies pour public.profiles
    DROP POLICY IF EXISTS "Service role bypass profiles" ON public.profiles;
    CREATE POLICY "Service role bypass profiles"
    ON public.profiles FOR ALL
    TO service_role, postgres
    USING (true)
    WITH CHECK (true);
    
    DROP POLICY IF EXISTS "Users can view own profile" ON public.profiles;
    CREATE POLICY "Users can view own profile"
    ON public.profiles FOR SELECT
    TO authenticated
    USING (auth.uid() = id);
    
    DROP POLICY IF EXISTS "Users can update own profile" ON public.profiles;
    CREATE POLICY "Users can update own profile"
    ON public.profiles FOR UPDATE
    TO authenticated
    USING (auth.uid() = id)
    WITH CHECK (auth.uid() = id);
    
    DROP POLICY IF EXISTS "Users can insert own profile" ON public.profiles;
    CREATE POLICY "Users can insert own profile"
    ON public.profiles FOR INSERT
    TO authenticated
    WITH CHECK (auth.uid() = id);
    
    -- Policies pour api_keys
    DROP POLICY IF EXISTS "Service role bypass api keys" ON public.api_keys;
    CREATE POLICY "Service role bypass api keys"
    ON public.api_keys FOR ALL
    TO service_role, postgres
    USING (true)
    WITH CHECK (true);
    
    DROP POLICY IF EXISTS "Users can manage own api keys" ON public.api_keys;
    CREATE POLICY "Users can manage own api keys"
    ON public.api_keys FOR ALL
    TO authenticated
    USING (user_id = auth.uid())
    WITH CHECK (user_id = auth.uid());
    
    -- Policies pour user_api_keys
    DROP POLICY IF EXISTS "Service role bypass user api keys" ON public.user_api_keys;
    CREATE POLICY "Service role bypass user api keys"
    ON public.user_api_keys FOR ALL
    TO service_role, postgres
    USING (true)
    WITH CHECK (true);
    
    DROP POLICY IF EXISTS "Users can manage own user api keys" ON public.user_api_keys;
    CREATE POLICY "Users can manage own user api keys"
    ON public.user_api_keys FOR ALL
    TO authenticated
    USING (user_id = auth.uid())
    WITH CHECK (user_id = auth.uid());
    
    -- Policies pour meetings
    DROP POLICY IF EXISTS "Service role bypass meetings" ON public.meetings;
    CREATE POLICY "Service role bypass meetings"
    ON public.meetings FOR ALL
    TO service_role, postgres
    USING (true)
    WITH CHECK (true);
    
    DROP POLICY IF EXISTS "Users can manage own meetings" ON public.meetings;
    CREATE POLICY "Users can manage own meetings"
    ON public.meetings FOR ALL
    TO authenticated
    USING (user_id = auth.uid())
    WITH CHECK (user_id = auth.uid());
    
    -- Policies pour storage.buckets
    DROP POLICY IF EXISTS "Service role bypass" ON storage.buckets;
    CREATE POLICY "Service role bypass" 
    ON storage.buckets FOR ALL 
    TO service_role, postgres
    USING (true)
    WITH CHECK (true);
    
    DROP POLICY IF EXISTS "Authenticated users can view buckets" ON storage.buckets;
    CREATE POLICY "Authenticated users can view buckets" 
    ON storage.buckets FOR SELECT 
    TO authenticated
    USING (true);
    
    -- Policies pour storage.objects
    IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = 'storage' AND table_name = 'objects') THEN
        -- Service role bypass pour tous les objets (CRITIQUE pour Storage service)
        DROP POLICY IF EXISTS "Service role bypass" ON storage.objects;
        DROP POLICY IF EXISTS "service_role_all" ON storage.objects;
        CREATE POLICY "service_role_all" 
        ON storage.objects FOR ALL 
        TO service_role, postgres, supabase_storage_admin
        USING (true)
        WITH CHECK (true);
        
        -- Nettoyer les anciennes policies
        DROP POLICY IF EXISTS "Users can upload to recordings" ON storage.objects;
        DROP POLICY IF EXISTS "Users can view own recordings" ON storage.objects;
        DROP POLICY IF EXISTS "Users can delete own recordings" ON storage.objects;
        DROP POLICY IF EXISTS "Public buckets are viewable" ON storage.objects;
        DROP POLICY IF EXISTS "Authenticated users can upload" ON storage.objects;
        DROP POLICY IF EXISTS "Authenticated users can select" ON storage.objects;
        DROP POLICY IF EXISTS "Authenticated users can delete" ON storage.objects;
        DROP POLICY IF EXISTS "authenticated_insert_all" ON storage.objects;
        DROP POLICY IF EXISTS "authenticated_select_all" ON storage.objects;
        DROP POLICY IF EXISTS "authenticated_update_all" ON storage.objects;
        DROP POLICY IF EXISTS "authenticated_delete_all" ON storage.objects;
        DROP POLICY IF EXISTS "anon_select_public" ON storage.objects;
        
        -- Policies ULTRA-PERMISSIVES pour authenticated (pas de vÃ©rification de bucket ou UUID)
        CREATE POLICY "authenticated_insert_all" 
        ON storage.objects FOR INSERT 
        TO authenticated
        WITH CHECK (true);

        CREATE POLICY "authenticated_select_all" 
        ON storage.objects FOR SELECT 
        TO authenticated
        USING (true);

        CREATE POLICY "authenticated_update_all" 
        ON storage.objects FOR UPDATE 
        TO authenticated
        USING (true)
        WITH CHECK (true);

        CREATE POLICY "authenticated_delete_all" 
        ON storage.objects FOR DELETE 
        TO authenticated
        USING (true);
        
        -- Public buckets viewable by anon
        CREATE POLICY "anon_select_public"
        ON storage.objects FOR SELECT
        TO anon
        USING (bucket_id IN (SELECT id FROM storage.buckets WHERE public = TRUE));
    END IF;
END \$\$;

SELECT 'DonnÃ©es initiales crÃ©Ã©es' as status;
EOF
else
    print_warning "Les tables n'ont pas Ã©tÃ© crÃ©Ã©es automatiquement"
fi

print_header "Ã‰TAPE 11/11 : VÃ©rification du dÃ©ploiement"

# VÃ©rifier l'Ã©tat des services
print_info "Ã‰tat des services :"
docker compose -f docker-compose.monorepo.yml ps

# VÃ©rifier les donnÃ©es crÃ©Ã©es avec diagnostic
print_info "VÃ©rification des donnÃ©es crÃ©Ã©es..."

USERS_COUNT=$(docker exec antislash-talk-db psql -U postgres -t -c "SELECT count(*) FROM auth.users;" 2>/dev/null | tr -d ' ')
PROFILES_COUNT=$(docker exec antislash-talk-db psql -U postgres -t -c "SELECT count(*) FROM public.profiles;" 2>/dev/null | tr -d ' ')
BUCKETS_COUNT=$(docker exec antislash-talk-db psql -U postgres -t -c "SELECT count(*) FROM storage.buckets;" 2>/dev/null | tr -d ' ')

echo ""
echo "ğŸ“Š RÃ©sultats de la vÃ©rification :"
echo "  - Utilisateurs (auth.users) : ${USERS_COUNT:-0}"
echo "  - Profils (public.profiles) : ${PROFILES_COUNT:-0}"
echo "  - Buckets (storage.buckets) : ${BUCKETS_COUNT:-0}"
echo ""

# VÃ©rifier si tout est OK
DEPLOYMENT_OK=true

if [ "${USERS_COUNT:-0}" -eq 0 ]; then
    print_error "âŒ PROBLÃˆME : Aucun utilisateur crÃ©Ã© dans auth.users"
    DEPLOYMENT_OK=false
    echo "   â†’ Diagnostic :"
    docker logs antislash-talk-auth --tail 20 2>&1 | grep -i error || echo "     Pas d'erreur Ã©vidente dans les logs Auth"
fi

if [ "${PROFILES_COUNT:-0}" -eq 0 ]; then
    print_error "âŒ PROBLÃˆME : Aucun profil crÃ©Ã© dans public.profiles"
    DEPLOYMENT_OK=false
    echo "   â†’ VÃ©rification : La table profiles existe-t-elle ?"
    docker exec antislash-talk-db psql -U postgres -t -c "\d public.profiles" 2>&1 | head -5
fi

if [ "${BUCKETS_COUNT:-0}" -eq 0 ]; then
    print_error "âŒ PROBLÃˆME : Aucun bucket crÃ©Ã© dans storage.buckets"
    DEPLOYMENT_OK=false
    echo "   â†’ Diagnostic :"
    docker logs antislash-talk-storage --tail 20 2>&1 | grep -i error || echo "     Pas d'erreur Ã©vidente dans les logs Storage"
fi

# Afficher les dÃ©tails si tout est OK
if [ "$DEPLOYMENT_OK" = true ]; then
    print_success "âœ… Toutes les vÃ©rifications sont OK !"
    echo ""
    print_info "DÃ©tails des donnÃ©es crÃ©Ã©es :"
    docker exec antislash-talk-db psql -U postgres -c "
SELECT 'Utilisateurs' as type, count(*) as count, string_agg(email, ', ') as details FROM auth.users
UNION ALL
SELECT 'Buckets' as type, count(*) as count, string_agg(name, ', ') as details FROM storage.buckets;"
fi

# Configuration Nginx pour Studio
print_info "Configuration de l'authentification Studio..."

# Nettoyer le fichier local s'il existe comme rÃ©pertoire
if [ -d "studio.htpasswd" ]; then
    rm -rf studio.htpasswd
fi

# GÃ©nÃ©rer le hash du mot de passe
STUDIO_PASSWORD_HASH=$(docker run --rm httpd:alpine htpasswd -nbB antislash "$STUDIO_PASSWORD" | cut -d: -f2)

# CrÃ©er le fichier .htpasswd directement dans le container (pas de montage de volume)
docker exec antislash-talk-studio-proxy sh -c "echo 'antislash:$STUDIO_PASSWORD_HASH' > /etc/nginx/.htpasswd && chmod 644 /etc/nginx/.htpasswd"

# VÃ©rifier que c'est bien un fichier
if docker exec antislash-talk-studio-proxy test -f /etc/nginx/.htpasswd; then
    print_success "Fichier .htpasswd crÃ©Ã© correctement dans le container"
    # Recharger nginx
    docker exec antislash-talk-studio-proxy nginx -s reload
else
    print_error "Erreur: .htpasswd n'a pas Ã©tÃ© crÃ©Ã© correctement"
    # Essayer de redÃ©marrer le container
    docker restart antislash-talk-studio-proxy
fi

# Afficher les informations finales
print_header "ğŸ‰ DÃ‰PLOIEMENT TERMINÃ‰ AVEC SUCCÃˆS !"

echo -e "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${GREEN}â•‘                     INFORMATIONS D'ACCÃˆS                       â•‘${NC}"
echo -e "${GREEN}â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£${NC}"
echo -e "${GREEN}â•‘${NC} Application Web    : ${CYAN}https://${VPS_HOST}${NC}"
echo -e "${GREEN}â•‘${NC} Supabase Studio    : ${CYAN}https://${VPS_HOST}:8444${NC}"
echo -e "${GREEN}â•‘${NC}   Utilisateur      : ${YELLOW}antislash${NC}"
echo -e "${GREEN}â•‘${NC}   Mot de passe     : ${YELLOW}${STUDIO_PASSWORD}${NC}"
echo -e "${GREEN}â•‘${NC}"
echo -e "${GREEN}â•‘${NC} API Supabase       : ${CYAN}https://${VPS_HOST}:8443${NC}"
echo -e "${GREEN}â•‘${NC} Ollama API         : ${CYAN}https://${VPS_HOST}:8445${NC}"
echo -e "${GREEN}â•‘${NC}"
echo -e "${GREEN}â•‘${NC} Compte Admin App   :"
echo -e "${GREEN}â•‘${NC}   Email            : ${YELLOW}${APP_USER_EMAIL}${NC}"
echo -e "${GREEN}â•‘${NC}   Mot de passe     : ${YELLOW}${APP_USER_PASSWORD}${NC}"
if [ -n "$HUGGINGFACE_TOKEN" ]; then
echo -e "${GREEN}â•‘${NC} HuggingFace Token  : ${YELLOW}${HUGGINGFACE_TOKEN:0:10}...${NC}"
fi
echo -e "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"

print_info "Toutes les informations ont Ã©tÃ© sauvegardÃ©es dans deployment-info.txt"

# Sauvegarder les informations
cat > deployment-info.txt << EOF
DÃ©ploiement Antislash Talk - $(date)
=====================================

URLs d'accÃ¨s :
- Application : https://${VPS_HOST} (HTTPS)
- Studio : https://${VPS_HOST}:8444 (HTTPS, user: antislash, pass: ${STUDIO_PASSWORD})
- API : https://${VPS_HOST}:8443 (HTTPS)
- Ollama : https://${VPS_HOST}:8445 (HTTPS)

Compte admin :
- Email : ${APP_USER_EMAIL}
- Mot de passe : ${APP_USER_PASSWORD}

Variables d'environnement :
- JWT_SECRET : ${JWT_SECRET:0:20}...
- ANON_KEY : ${ANON_KEY:0:20}...
- SERVICE_ROLE_KEY : ${SERVICE_ROLE_KEY:0:20}...
- POSTGRES_PASSWORD : ${POSTGRES_PASSWORD}
EOF

if [ -n "$HUGGINGFACE_TOKEN" ]; then
    echo "- HUGGINGFACE_TOKEN : ${HUGGINGFACE_TOKEN:0:10}..." >> deployment-info.txt
fi

# Afficher le rÃ©sultat final
echo ""
echo "=========================================="
if [ "$DEPLOYMENT_OK" = true ]; then
    print_success "ğŸ‰ DÃ‰PLOIEMENT TERMINÃ‰ AVEC SUCCÃˆS !"
    echo ""
    echo "âœ… Tous les services sont opÃ©rationnels"
    echo "âœ… ${USERS_COUNT} utilisateur(s) crÃ©Ã©(s)"
    echo "âœ… ${PROFILES_COUNT} profil(s) crÃ©Ã©(s)"
    echo "âœ… ${BUCKETS_COUNT} bucket(s) crÃ©Ã©(s)"
else
    print_error "âš ï¸  DÃ‰PLOIEMENT TERMINÃ‰ AVEC DES PROBLÃˆMES"
    echo ""
    echo "Des erreurs ont Ã©tÃ© dÃ©tectÃ©es. Solutions possibles :"
    echo ""
    echo "1. Relancer le script de correction manuelle :"
    echo "   ./fix-complete-deployment.sh"
    echo ""
    echo "2. VÃ©rifier les logs des services problÃ©matiques :"
    echo "   docker logs antislash-talk-auth --tail 50"
    echo "   docker logs antislash-talk-storage --tail 50"
    echo ""
    echo "3. RedÃ©ployer complÃ¨tement depuis zÃ©ro :"
    echo "   ./clean-and-deploy.sh"
    echo ""
    echo "4. Diagnostic dÃ©taillÃ© de la base de donnÃ©es :"
    echo "   docker exec -it antislash-talk-db psql -U postgres -d postgres"
    echo "   Puis exÃ©cuter : \\dt auth.*"
    echo "                   \\dt storage.*"
    echo "                   \\dt public.*"
    echo ""
fi
echo "=========================================="
echo ""

exit $([ "$DEPLOYMENT_OK" = true ] && echo 0 || echo 1)
